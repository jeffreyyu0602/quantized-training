import logging
import re
from collections import defaultdict
from typing import Dict, Tuple, Callable, Any, Union, List

import torch
from torch.ao.quantization.fx.utils import assert_and_get_unique_device
from torch.fx import GraphModule, Node
from torch.fx.graph import map_arg
from accelerate.big_modeling import infer_auto_device_map
from accelerate.utils import get_max_memory

from .quantize_pt2e import export_model


logger = logging.getLogger(__name__)


__all__ = [
    "deduplicate_nodes",
    "dispatch_model",
    "dtype_byte_size",
    "get_device_map",
    "get_aten_graph_module",
    "get_node_name_to_scope",
    "insert_align_device_nodes",
    "print_node_scope_tabular",
    "sink_obs_or_fq",
]


def dtype_byte_size(dtype: torch.dtype):
    """
    Returns the size (in bytes) occupied by one parameter of type `dtype`.

    Example:

    ```py
    >>> dtype_byte_size(torch.float32)
    4
    ```
    """
    if dtype == torch.bool:
        return 1 / 8
    bit_search = re.search(r"[^\d](\d+)(_.*)?$", str(dtype))
    if bit_search is None:
        raise ValueError(f"`dtype` is not a valid dtype: {dtype}.")
    bit_width = int(bit_search.groups()[0])
    return bit_width / 8.0


def fetch_attr(module, target):
    target_atoms = target.split('.')
    attr_itr = module
    for i, atom in enumerate(target_atoms):
        if not hasattr(attr_itr, atom):
            raise RuntimeError(f"Node referenced nonexistant target {'.'.join(target_atoms[:i])}")
        attr_itr = getattr(attr_itr, atom)
    return attr_itr


def get_device_map(model: GraphModule, max_memory=None, verbose=False):
    """
    The default device map generated by the Hugging Face accelerate library is
    suboptimal because it places all tensors in the order of model parameters,
    children, and buffers, which may not be used in the same order. This
    function generates a device map based on the order of nodes in the graph,
    which reflects the exact order in which tensors are used.
    """
    if max_memory is None:
        max_memory = get_max_memory(max_memory)

    devices = list(max_memory.keys())
    if "disk" not in devices:
        devices.append("disk")

    device_map = {}
    current_device = 0
    current_memory_used = 0

    named_modules = dict(model.named_modules(remove_duplicate=True))

    first_use : Dict[Node, int] = {}

    def register_first_use(n: Node, idx: int):
        if n not in first_use:
            first_use[n] = idx

    for i, node in enumerate(model.graph.nodes):
        map_arg(node.args, lambda n: register_first_use(n, i))
        map_arg(node.kwargs, lambda n: register_first_use(n, i))

    nodes_to_treat = [n for n in model.graph.nodes if n.op in ['get_attr', 'call_module']]
    nodes_to_treat = sorted(nodes_to_treat, key=lambda n: first_use[n])

    while len(nodes_to_treat) > 0:
        node = nodes_to_treat.pop(0)
        if verbose:
            print(f"\nTreating node {node}.")
        # Assess size needed
        if node.op == 'get_attr':
            tensor = fetch_attr(model, node.target)
            if not isinstance(tensor, torch.Tensor):
                continue
            module_size = tensor.numel() * dtype_byte_size(tensor.dtype)
        else:
            module = named_modules[node.target]
            module_size = (
                sum(p.numel() * dtype_byte_size(p.dtype) for p in module.parameters()) +
                sum(b.numel() * dtype_byte_size(b.dtype) for b in module.buffers())
            )

        device = devices[current_device]
        current_max_size = max_memory[device] if device != "disk" else None

        if current_max_size is not None and current_memory_used + module_size > current_max_size:
            if verbose:
                print(
                    f"Not enough space on {devices[current_device]} to put {node} (space available "
                    f"{current_max_size-current_memory_used}, module size {module_size})."
                )
            current_device += 1
            assert current_device < len(devices), "Not enough devices to store the model."
            nodes_to_treat = [node] + nodes_to_treat
            current_memory_used = 0
        else:
            if verbose:
                if current_max_size is None:
                    print(f"Putting {node} (size={module_size}) on {devices[current_device]}.")
                else:
                    print(
                        f"Putting {node} (size={module_size}) on {devices[current_device]} "
                        f"(available={current_max_size-current_memory_used})."
                    )
            current_memory_used += module_size
            device_map[node.target] = devices[current_device]

    return device_map


def dispatch_model(model, device_map=None, max_memory=None):
    if device_map is None:
        device_map = infer_auto_device_map(model, max_memory)

    for name, param in model.named_parameters():
        param.data = param.data.to(device_map[name])

    for name, child in model.named_children():
        if name in device_map:
            child.to(device_map[name])

    for name, buffer in model.named_buffers(recurse=False):
        if name not in device_map:
            logger.warning(f"Buffer {name} not found in device_map, skipping.")
            continue
        buffer.data = buffer.data.to(device_map[name])


def deduplicate_nodes(graph: torch.fx.Graph):
    """
    Deduplicate identical nodes in a torch.fx.Graph.
    Nodes are considered identical if:
      - They have the same op type
      - They have the same target
      - They have the same args and kwargs
    """
    seen = {}
    mapping = {}

    for node in list(graph.nodes):
        if node.op in ("placeholder", "output"):
            continue

        key = (node.op, node.target, tuple(node.args), frozenset(node.kwargs.items()))

        if key in seen:
            # Redirect all users of the duplicate to the original
            orig = seen[key]
            for user in list(node.users):
                user.replace_input_with(node, orig)
            # Remove duplicate from graph
            graph.erase_node(node)
            mapping[node] = orig
        else:
            seen[key] = node

    for old, new in mapping.items():
        logger.info(f"Deduplicated {old} to {new}")

    graph.lint()
    return mapping


def sink_obs_or_fq(model: GraphModule) -> GraphModule:
    graph = model.graph

    def is_obs_or_fq(node):
        return (
            node.op == "call_module"
            and "activation_post_process" in node.target
        )

    for node in reversed(graph.nodes):
        if not is_obs_or_fq(node):
            continue

        input_node = node.args[0]

        # Handle double quantization case where input is also an obs or fq
        if is_obs_or_fq(input_node):
            input_node = input_node.args[0]

        if input_node.op != "get_attr":
            continue

        order = {n: i for i, n in enumerate(graph.nodes)}
        users = list(node.users.keys())
        first_user = min(users, key=lambda n: order[n])

        with graph.inserting_before(first_user):
            new_fq = graph.node_copy(node, lambda x: x)

        logger.info(f"Replacing {node} with {new_fq} before {first_user}")

        node.replace_all_uses_with(new_fq)
        graph.erase_node(node)

    graph.lint()
    model.recompile()
    return model


@torch.no_grad()
def insert_align_device_nodes(model, example_args):
    args_iter = iter(example_args)
    env : Dict[str, Node] = {}
    modules = dict(model.named_modules())

    deduplicate_nodes(model.graph)

    node_to_last_use : Dict[Node, Node] = {}
    user_to_last_uses : Dict[Node, List[Node]] = {}

    def register_last_uses(n: Node, user: Node):
        if n not in node_to_last_use:
            node_to_last_use[n] = user
            user_to_last_uses.setdefault(user, []).append(n)

    for node in reversed(model.graph.nodes):
        map_arg(node.args, lambda n: register_last_uses(n, node))
        map_arg(node.kwargs, lambda n: register_last_uses(n, node))

    def delete_unused_values(user : Node):
        """
        Delete values after their last use. This ensures that values that are
        not used in the remainder of the code are freed and the memory usage
        of the code is optimal.
        """
        nodes_to_delete = user_to_last_uses.get(user, [])
        for n in nodes_to_delete:
            env.pop(n.name, None)

    def load_arg(a):
        return torch.fx.graph.map_arg(a, lambda n: env[n.name])

    def get_unique_device(nodes):
        args = load_arg(nodes)
        devices = {t.device for t in args if isinstance(t, torch.Tensor)}
        non_cpu_devices = [d for d in devices if d.type != 'cpu']
        return non_cpu_devices[-1] if non_cpu_devices else None

    def insert_adaptor(node, user, device):
        value = env[node.name]
        if not isinstance(value, torch.Tensor) or value.device == device:
            return

        to_node = next((
            n for n in node.users if n.target == torch.Tensor.to and n.args[1] == device
        ), None)

        if to_node is None:
            with model.graph.inserting_after(node):
                to_node = model.graph.call_function(torch.Tensor.to, (node, device))

        user.replace_input_with(node, to_node)

        if node_to_last_use[node] == user:
            env.pop(node.name)

        env.setdefault(to_node.name, value.to(device))

        # Update last uses
        node_to_last_use.clear()
        user_to_last_uses.clear()

        for n in reversed(model.graph.nodes):
            map_arg(n.args, lambda arg: register_last_uses(arg, n))
            map_arg(n.kwargs, lambda kwarg: register_last_uses(kwarg, n))

    for node in model.graph.nodes:
        try:
            if node.op == 'placeholder':
                result = next(args_iter, None)
            elif node.op == 'get_attr':
                result = fetch_attr(model, node.target)
            elif node.op == 'call_function':
                device = get_unique_device(node.all_input_nodes)
                if device is not None:
                    for n in node.all_input_nodes:
                        insert_adaptor(n, node, device)
                result = node.target(*load_arg(node.args), **load_arg(node.kwargs))
            elif node.op == 'call_module':
                module = modules[node.target]
                device = assert_and_get_unique_device(module)
                for n in node.all_input_nodes:
                    insert_adaptor(n, node, device)
                result = modules[node.target](*load_arg(node.args), **load_arg(node.kwargs))
        except:
            print(f"Error processing node {node} of type {node.op} with target {node.target}")
            raise

        env[node.name] = result
        delete_unused_values(node)

    model.graph.lint()
    model.recompile()


def get_aten_graph_module(
    pattern: Callable,
    example_inputs: Tuple[Any, ...],
    example_kwargs: Dict[str, Any] = None,
    dynamic_shapes: Union[Dict[str, Any], Tuple[Any], None] = None,
    is_cuda: bool = False,
) -> GraphModule:
    """
    Convert the pattern to an FX graph with decomposed aten ops.
    """
    if is_cuda:
        example_inputs = tuple([x.cuda() if isinstance(x, torch.Tensor) else x for x in example_inputs])
    aten_pattern = export_model(
        pattern,
        example_inputs,
        example_kwargs,
        dynamic_shapes=dynamic_shapes,
    )
    aten_pattern.graph.eliminate_dead_code()
    aten_pattern.recompile()
    return aten_pattern


def get_node_name_to_scope(model: GraphModule) -> Dict[str, Tuple[str, type, int]]:
    node_name_to_scope: Dict[str, Tuple[str, type]] = {}
    submodule_to_object_type_to_cur_idx: Dict[str, Dict[Callable, int]] = \
        defaultdict(lambda: defaultdict(int))
    for n in model.graph.nodes:
        if (nn_module_stack := n.meta.get("nn_module_stack", None)) is None:
            node_name_to_scope[n.name] = [("", type(None))]
            continue

        current_scope = []
        for bt in nn_module_stack.values():
            module_path = bt[0]
            cur_object_type_idx = \
                submodule_to_object_type_to_cur_idx[module_path][n.target]
            submodule_to_object_type_to_cur_idx[module_path][n.target] += 1
            current_scope.append((module_path, bt[1], cur_object_type_idx))
        node_name_to_scope[n.name] = current_scope[-1]

    return node_name_to_scope


def print_node_scope_tabular(gm: GraphModule):
    try:
        from tabulate import tabulate
    except ImportError:
        print("`print_tabular` relies on the library `tabulate`, "
                "which could not be found on this machine. Run `pip "
                "install tabulate` to install the library.")
        raise

    node_name_to_scope = get_node_name_to_scope(gm)
    node_specs = [
        [n.op, n.name, n.target, node_name_to_scope[n.name]]
        for n in gm.graph.nodes if n.name in node_name_to_scope
    ]
    print(tabulate(node_specs,
                   headers=['opcode', 'name', 'target', 'scope']))
